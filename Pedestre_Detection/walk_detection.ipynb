{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importe os pacotes necessários:\n",
    "- O Numpy é usado para armazenar todos os dados, recuperá-los do modelo e trabalhar com eles.\n",
    "\n",
    "- O OpenCV é usado para ler quadros de um arquivo de vídeo ou da transmissão da webcam, redimensioná-los e remodelá-los de acordo com os requisitos do modelo. Ele também fornece o módulo dnn, que usaremos para trabalhar com nosso modelo de rede neural profunda. Além disso, ele desenha as caixas delimitadoras na imagem e as mostra ao usuário.\n",
    "\n",
    "- O Os é usado para trabalhar com arquivos, ler caminhos e outras operações relacionadas a arquivos.\n",
    "\n",
    "- O Imutils é outra ótima biblioteca para realizar diferentes ações em imagens. Ele atua como uma biblioteca auxiliar fornecendo algumas funções muito úteis para o OpenCV, que já é uma biblioteca vasta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import imutils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://data-flair.training/blogs/pedestrian-detection-python-opencv/\n",
    "\n",
    "https://github.com/adityagandhamal/tensorflow-object-detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declarando algumas variáveis de limiar que usaremos posteriormente. O limiar NMS é o limite para separar previsões sobrepostas, mais sobre NMS adiante. MinConfiança é o limite para a pontuação de confiança retornada pelo modelo acima do qual uma previsão é considerada verdadeira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMS_THRESHOLD=0.3\n",
    "MIN_CONFIDENCE=0.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Função de Detecção de Pedestres\n",
    "\n",
    "Agora vamos definir a função mais importante que usaremos para a detecção de pedestres. Ela recebe o quadro da imagem, quadro a quadro, do OpenCV, seja da webcam ou de um arquivo de vídeo, o modelo, o nome da camada de saída da qual obteremos o resultado e uma variável \"personidz\".\n",
    "Obtemos as dimensões do quadro passado e inicializamos uma matriz que armazenará o resultado do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pedestrian_detection(image, model, layer_name, personidz=0):\n",
    "\t(H, W) = image.shape[:2]\n",
    "\tresults = []\n",
    "\n",
    "\t# Construímos um blob com o quadro que recebemos e, em seguida, o passamos para o modelo YOLO, executando uma passagem direta \n",
    " \t# (forward pass), que retornará as caixas delimitadoras das detecções e as pontuações de confiança para cada detecção. O model.forward \n",
    " \t# retornará a saída da camada que foi passada para ele, ou seja, a camada de saída.\n",
    "\tblob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
    "\t\tswapRB=True, crop=False)\n",
    "\tmodel.setInput(blob)\n",
    "\tlayerOutputs = model.forward(layer_name)\n",
    "\n",
    "\t# Crie arrays para armazenar as caixas delimitadoras resultantes, centroides e as pontuações de confiança associadas.\n",
    "\tboxes = []\n",
    "\tcentroids = []\n",
    "\tconfidences = []\n",
    "\n",
    "\t# layerOutputs é uma lista de listas que contém as saídas. Cada lista em layerOutputs contém detalhes sobre uma única \n",
    " \t# previsão, como a caixa delimitadora, a confiança, etc. Portanto, todas as previsões estão agrupadas em uma lista de listas. \n",
    " \t# Percorremos essa estrutura usando um loop.\n",
    "\tfor output in layerOutputs:\n",
    "\t\tfor detection in output:\n",
    "\n",
    "\t\t\t# A partir da detecção individual, obtemos as pontuações para todas as previsões, o ID da classe e a confiança para o \n",
    "   \t\t\t# ID da classe com pontuação máxima.\n",
    "\t\t\tscores = detection[5:]\n",
    "\t\t\tclassID = np.argmax(scores)\n",
    "\t\t\tconfidence = scores[classID]\n",
    "   \n",
    "   \n",
    "\t\t\t# Obtendo a Detecção\n",
    "\t\t\t# Agora precisamos obter a detecção que desejamos. O ID da classe para detectar uma pessoa é 0 no formato YOLO, portanto, \n",
    "   \t\t\t# verificamos isso e também se a confiança está acima do limite para evitar falsos positivos.\n",
    "\t\t\t# O YOLO retorna o centróide da caixa delimitadora com a altura e largura da caixa, em vez da coordenada superior esquerda da caixa.\n",
    "\t\t\tif classID == personidz and confidence > MIN_CONFIDENCE:\n",
    "\n",
    "\t\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
    "\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t\t# Agora, como não temos as coordenadas do canto superior direito da caixa delimitadora, vamos calculá-las subtraindo \n",
    "    \t\t\t# metade da largura e altura do ponto x do centróide e ponto y do centróide, respectivamente.\n",
    "\t\t\t\t# Agora que temos todas as informações de que precisamos, vamos adicioná-las às listas que criamos anteriormente.\n",
    "\t\t\t\tx = int(centerX - (width / 2))\n",
    "\t\t\t\ty = int(centerY - (height / 2))\n",
    "\n",
    "\t\t\t\tboxes.append([x, y, int(width), int(height)])\n",
    "\t\t\t\tcentroids.append((centerX, centerY))\n",
    "\t\t\t\tconfidences.append(float(confidence))\n",
    "    \n",
    "\t\n",
    "\t# Out of the loop here we do nms or non maxima suppression to remove overlapping and weak bounding boxes. Because neighboring \n",
    " \t# windows of the actual detection also contain high scores, hundreds of predictions pop up that we need to clean. We use the opencv’s \n",
    " \t# dnn module provided nmsboxes function for this\n",
    "\tidzs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONFIDENCE, NMS_THRESHOLD)\n",
    " \n",
    "\t# Verifique se temos alguma detecção e percorra-a. Extraia as coordenadas da caixa delimitadora, a largura e altura, \n",
    " \t# e adicione todos os detalhes à nossa lista de resultados que criamos anteriormente. Por fim, retorne essa lista e encerre a função.\n",
    "\tif len(idzs) > 0:\n",
    "\t\tfor i in idzs.flatten():\n",
    "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
    "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
    "\t\t\tres = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
    "\t\t\tresults.append(res)\n",
    "   \n",
    "   \n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui carregamos os rótulos nos quais o modelo foi treinado. Ele possui 80 rótulos de classes no arquivo coco.names no formato COCO.\n",
    "# Defina o caminho para os arquivos de pesos (weights) e de configuração (cfg) para o nosso modelo. Lembre-se de que esses são caminhos\n",
    "# relativos e, neste caso, como os arquivos estão na mesma pasta, usamos apenas o nome como caminhos.\n",
    "\n",
    "labelsPath = \"names/coco.names\"\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "weights_path = \"yolo/yolov4-tiny.weights\"\n",
    "config_path = \"yolo/yolov4-tiny.cfg\"\n",
    "\n",
    "model = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "\n",
    "# If your Opencvdnn compiled with cuda and you want to run this on the GPU uncomment these lines to use cuda enabled GPUs.\n",
    "model.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "model.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "# Precisamos do nome da última camada de aprendizado profundo para obter a saída dela. Em seguida, inicializamos o VideoCapture do OpenCV.\n",
    "# Se você deseja usar a webcam, passe 0 em vez do nome do arquivo para a função VideoCapture.\n",
    "layer_name = model.getLayerNames()\n",
    "layer_name = [layer_name[i - 1] for i in model.getUnconnectedOutLayers()]\n",
    "cap = cv2.VideoCapture(\"videos/Test_2.mp4\")\n",
    "writer = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lendo os quadros e processando-os\n",
    "- Agora, inicie um loop infinito e leia os quadros do VideoCapture. Se os quadros acabarem, ou seja, o vídeo terminar, saímos do loop infinito.\n",
    "\n",
    "- Redimensione a imagem de acordo com os requisitos sem alterar a proporção da imagem usando a função resize do imutils.\n",
    "\n",
    "- Passe a imagem, o nome da camada de saída do modelo e o ID da classe para a etiqueta \"person\" para a função pedestrian_detection que criamos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@371.576] global net_impl.cpp:174 setUpNet DNN module was not built with CUDA backend; switching to CPU\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "\t(grabbed, image) = cap.read()\n",
    "\n",
    "\tif not grabbed:\n",
    "\t\tbreak\n",
    "\timage = imutils.resize(image, width=700)\n",
    "\tresults = pedestrian_detection(image, model, layer_name,\n",
    "\t\tpersonidz=LABELS.index(\"person\"))\n",
    "\n",
    "\t# Obtendo os resultados da função, percorremos os resultados e desenhamos retângulos de caixa delimitadora nas previsões e mostramos \n",
    " \t# ao usuário. Capturamos a tecla ESC pressionada com a função waitKey do OpenCV e saímos do loop. Por fim, liberamos a captura e fechamos \n",
    " \t# todas as janelas.\n",
    "\tfor res in results:\n",
    "\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 255, 0), 2)\n",
    "\n",
    "\tcv2.imshow(\"Detection\",image)\n",
    "\n",
    "\tkey = cv2.waitKey(1)\n",
    "\tif key == 27:\n",
    "\t\tbreak\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands Position Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     success, img \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[0;32m---> 16\u001b[0m     imgRGB \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(img, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2RGB)\n\u001b[1;32m     17\u001b[0m     results \u001b[39m=\u001b[39m hands\u001b[39m.\u001b[39mprocess(imgRGB)\n\u001b[1;32m     18\u001b[0m     \u001b[39m#print(results.multi_hand_landmarks)\u001b[39;00m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Install the Libraries\n",
    "\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "cap = cv2.VideoCapture(\"videos/hands.mp4\")\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands()\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "cTime = 0\n",
    "pTime = 0\n",
    "# Function Start\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(imgRGB)\n",
    "    #print(results.multi_hand_landmarks)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for handlms in results.multi_hand_landmarks:\n",
    "            for id, lm in enumerate(handlms.landmark):\n",
    "                #print(id, lm)\n",
    "                h, w, c = img.shape\n",
    "                cx, cy = int(lm.x*w), int(lm.y*h)\n",
    "                # print(id, cx, cy)\n",
    "                #if id == 5:\n",
    "                cv2.circle(img, (cx, cy), 15, (139, 0, 0), cv2.FILLED)\n",
    "\n",
    "\n",
    "            mpDraw.draw_landmarks(img, handlms, mpHands.HAND_CONNECTIONS)\n",
    "# Time and FPS Calculation\n",
    "\n",
    "    cTime = time.time()\n",
    "    fps = 1/(cTime-pTime)\n",
    "    pTime = cTime\n",
    "    \n",
    "    cv2.putText(img, str(int(fps)), (10,70), cv2.FONT_HERSHEY_SIMPLEX, 3, (139,0,0), 3)\n",
    "\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    cv2.waitKey(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
