{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2706,  0.2706,  0.2706,  ...,  0.3333,  0.2157,  0.0039],\n",
       "         [ 0.2706,  0.2706,  0.2706,  ...,  0.3333,  0.2157,  0.0039],\n",
       "         [ 0.2706,  0.2706,  0.2706,  ...,  0.3333,  0.2157,  0.0039],\n",
       "         ...,\n",
       "         [-0.6627, -0.6627, -0.6078,  ..., -0.1843, -0.2157, -0.2314],\n",
       "         [-0.6549, -0.6549, -0.5686,  ..., -0.1843, -0.1765, -0.1529],\n",
       "         [-0.6549, -0.6549, -0.5686,  ..., -0.1843, -0.1765, -0.1529]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenna = Image.open(\"../../Pictures/Lenna.png\").convert(\"L\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((244, 244)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])\n",
    "\n",
    "lenna_tensor = transform(lenna)\n",
    "lenna_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(img, mean=0, std=0.1):\n",
    "    noise = torch.randn_like(img) * std + mean\n",
    "    return img + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenna_noise = add_noise(lenna_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sobel_filter():\n",
    "    sobel_tensor = torch.Tensor([[[[1, 2, 1], [0, 0, 0], [-1, -2, -1]]],[[[1, 0, -1], [2, 0, -2], [1, 0, -1]]]])\n",
    "    \n",
    "    conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1, bias=False)\n",
    "    conv.weight.data = sobel_tensor\n",
    "    \n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNoise(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNoise, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1, stride=1)\n",
    "        self.blur = transforms.GaussianBlur(kernel_size=3)\n",
    "        self.sobel = get_sobel_filter()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.blur(x)\n",
    "        x = self.sobel(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur_net = ConvNoise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenna_blur = blur_net(lenna_tensor)\n",
    "lenna_noise_blur = blur_net(lenna_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[1;32m      2\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m132\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m plt\u001b[39m.\u001b[39;49mimshow(lenna_blur\u001b[39m.\u001b[39;49mpermute(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m))\n\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mCom ruído\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m133\u001b[39m)\n",
      "File \u001b[0;32m~/faculdade/Computional Vision/.venv/lib/python3.10/site-packages/matplotlib/pyplot.py:2695\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2689\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[1;32m   2690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[1;32m   2691\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2692\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2693\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n\u001b[1;32m   2694\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2695\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39;49mimshow(\n\u001b[1;32m   2696\u001b[0m         X, cmap\u001b[39m=\u001b[39;49mcmap, norm\u001b[39m=\u001b[39;49mnorm, aspect\u001b[39m=\u001b[39;49maspect,\n\u001b[1;32m   2697\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation, alpha\u001b[39m=\u001b[39;49malpha, vmin\u001b[39m=\u001b[39;49mvmin,\n\u001b[1;32m   2698\u001b[0m         vmax\u001b[39m=\u001b[39;49mvmax, origin\u001b[39m=\u001b[39;49morigin, extent\u001b[39m=\u001b[39;49mextent,\n\u001b[1;32m   2699\u001b[0m         interpolation_stage\u001b[39m=\u001b[39;49minterpolation_stage,\n\u001b[1;32m   2700\u001b[0m         filternorm\u001b[39m=\u001b[39;49mfilternorm, filterrad\u001b[39m=\u001b[39;49mfilterrad, resample\u001b[39m=\u001b[39;49mresample,\n\u001b[1;32m   2701\u001b[0m         url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n\u001b[1;32m   2702\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2703\u001b[0m     sci(__ret)\n\u001b[1;32m   2704\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/faculdade/Computional Vision/.venv/lib/python3.10/site-packages/matplotlib/__init__.py:1442\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1441\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1442\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1444\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1445\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1446\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/faculdade/Computional Vision/.venv/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5665\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5657\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5658\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[1;32m   5659\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[1;32m   5660\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[1;32m   5661\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m   5662\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5663\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 5665\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[1;32m   5666\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5667\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5668\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/faculdade/Computional Vision/.venv/lib/python3.10/site-packages/matplotlib/image.py:697\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(A, PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage):\n\u001b[1;32m    696\u001b[0m     A \u001b[39m=\u001b[39m pil_to_array(A)  \u001b[39m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 697\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39;49msafe_masked_invalid(A, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    699\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39muint8 \u001b[39mand\u001b[39;00m\n\u001b[1;32m    700\u001b[0m         \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mcan_cast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, \u001b[39mfloat\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msame_kind\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m    701\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage data of dtype \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m cannot be converted to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype))\n",
      "File \u001b[0;32m~/faculdade/Computional Vision/.venv/lib/python3.10/site-packages/matplotlib/cbook/__init__.py:709\u001b[0m, in \u001b[0;36msafe_masked_invalid\u001b[0;34m(x, copy)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msafe_masked_invalid\u001b[39m(x, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 709\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(x, subok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    710\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39misnative:\n\u001b[1;32m    711\u001b[0m         \u001b[39m# If we have already made a copy, do the byteswap in place, else make a\u001b[39;00m\n\u001b[1;32m    712\u001b[0m         \u001b[39m# copy with the byte order swapped.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mbyteswap(inplace\u001b[39m=\u001b[39mcopy)\u001b[39m.\u001b[39mnewbyteorder(\u001b[39m'\u001b[39m\u001b[39mN\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Swap to native order.\u001b[39;00m\n",
      "File \u001b[0;32m~/faculdade/Computional Vision/.venv/lib/python3.10/site-packages/torch/_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    969\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    971\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(132)\n",
    "plt.imshow(lenna_blur.detach().numpy().permute(1, 2, 0))\n",
    "plt.title('Com ruído')\n",
    "plt.subplot(133)\n",
    "plt.imshow(lenna_noise_blur.detach().numpy().permute(1, 2, 0))\n",
    "plt.title('Filtrada')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
